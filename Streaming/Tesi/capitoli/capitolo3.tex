%license:BSD-3-Clause
%copyright-holders:Michele Maione
%============================================================
%
%	Piattaforma di cloud gaming per giochi arcade
%
%============================================================
\chapter{Implementazione}
%In questa sezione si spiega come è stato affrontato il problema concettualmente, la soluzione logica che ne è seguita senza la documentazione.
%Si mostra il progetto dell’architettura del sistema con i vari moduli.
In questo capitolo verranno descritte le cinque fasi aggiuntive del cloud gaming e la loro implementazione in C++ come nuovi moduli del MAME: la cattura audio-video, la codifica, la trasmissione, la decodifica e la gestione dell'input utente.




\section{Cattura}
Nel paradigma del cloud gaming per cattura si intende la fase in cui l'output audio-visivo del videogioco viene messo da parte per essere poi inviato al codificatore. Nei prossimi due paragrafi vedremo i metodi di cattura per la parte video e per quella audio.


\subsection{Cattura video} \label{subsec:cap3_Video}
Per la cattura video esistono tre metodi di cattura: l'hook di funzioni della libreria multimediale, l'utilizzo delle API per il framebuffer virtuale e l'uso di schede video con funzionalità di codifica incorporata.

L'hook di funzione è una tecnica di programmazione con la quale si sostituisce il comportamento di una funzione di un programma o di una libreria già compilata. Questa operazione su una libreria multimediale consiste nel modificare il normale funzionamento della procedura che invia il framebuffer della scheda video all'uscita. Ad esempio per le librerie multimediali più usate le funzioni da catturare sono: \verb|SDL_RenderPresent| per SDL, \verb|Present| per DirectX e \verb|SwapBuffer| per OpenGL \parencite{GamingAnywhere}. Questo metodo è stato usato ad esempio da OnLive, Gaikai e Vortex. Un esempio di codice per l'hook di funzioni per SDL è mostrato nel Lis. \ref{lst:hookCode}.

\begin{lstlisting}[caption=Codice di hook per la libreria SDL, label={lst:hookCode}]
	HOOK_TRACE_INFO hHook = { NULL };
	
	HMODULE lib_handle = LoadLibrary("SDL2.dll");

	FARPROC old_SDL_RenderPresent =	GetProcAddress(
		lib_handle, "SDL_RenderPresent");

	NTSTATUS result = LhInstallHook(
		old_SDL_RenderPresent, my_SDL_RenderPresent,
		NULL, &hHook);
\end{lstlisting}

Il framebuffer virtuale è una funzionalità offerta dalle API di sistema per accedere all'attuale framebuffer presente nella scheda video. Il loro utilizzo principale è per il remote desktop, ma alcune piattaforme come "GamingAnywhere" \parencite{GamingAnywhere} e "Parsec"\footnote{Parsec è un servizio di desktop remoto basato su Windows offerto dalla omonima società.} \parencite{TheTechnologyBehindALowLatencyCloudGamingService} utilizzano questa funzionalità per la cattura video, il primo per la piattaforma di cloud gaming, il secondo per offrire un computer virtuale. Alcuni esempi nei moderni sistemi operativi sono: "X virtual framebuffer" su Linux X11 \parencite{XVFB}, "Desktop Duplication API" su Windows \parencite{DesktopDuplicationAPI} e \verb|CGDisplayStream| della libreria "Quartz Display Services" su macOS.

Le schede video con codifica incorporata non hanno una fase di cattura poiché il chip di codifica legge i dati direttamente dal framebuffer; l'argomento è trattato più avanti nel paragrafo \ref{sec:cap3_Codifica}.

Poiché sono disponibili i sorgenti del progetto MAME in questo progetto ho scelto di modificare il comportamento normale del programma. Come mostrato in Fig. \ref{fig:class_renderingSDLFull_Streaming}, sono state aggiunte alla classe \verb|renderer_sdl2| le due funzioni (\verb|init_streaming_render| e \verb|free_streaming_render|) per allocare e deallocare le strutture dati per la cattura. Come mostrato nel Lis. \ref{lst:init_streaming_render_init} le funzioni della libreria SDL che sono state aggiunte al codice sono:

\begin{itemize}
	\item CreateRGBSurfaceWithFormat: crea una superficie RGB specificando il formato pixel da utilizzare;	
	\item CreateSoftwareRenderer: crea un contesto di rendering 2D per una superficie;
	\item RWFromMem: prepara un buffer di memoria di lettura-scrittura da utilizzare con la struttura dati RWops (read-write opaque pointer structure).	
\end{itemize}

\begin{lstlisting}[caption=Codice aggiunto per la cattura video: inizializzazione, label={lst:init_streaming_render_init}]
	sdl_buffer_bytes_length = w * h * 4; // RGBA = 4 bytes
	sdl_buffer_bytes = new char[sdl_buffer_bytes_length];

	sdl_surface = SDL_CreateRGBSurfaceWithFormat(
		0, w, h, 32, SDL_PIXELFORMAT_RGBA32);
	
		sdl_renderer = SDL_CreateSoftwareRenderer(sdl_surface);
	
	sdl_buffer = SDL_RWFromMem(
		sdl_buffer_bytes, sdl_buffer_bytes_length);
\end{lstlisting}

Nel Lis. \ref{lst:init_streaming_render_draw} è mostrato il codice che è stato inserito per inviare alla classe statica che gestisce lo streaming (\verb|streaming_server|) il frame da inviare al giocatore (che verrà codificato prima dell'invio). Poiché l'attuale contesto di rendering (\verb|sdl_renderer|) è creato su una superfice RGB (\verb|sdl_surface|) quando il contesto di rendering viene aggiornato con il framebuffer corrente (tramite \verb|SDL_RenderPresent|) è la superfice a ricevere il frame attuale.

\begin{lstlisting}[caption=Codice aggiunto per la cattura video: disegno, label={lst:init_streaming_render_draw}]
	SDL_RenderPresent(sdl_renderer);

	streaming_server::instance()
		.send_video_frame(sdl_surface->pixels);

	SDL_RWseek(sdl_buffer, 0, RW_SEEK_SET); // pos. inizio del buffer
\end{lstlisting}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{immagini/class_renderingSDLFull_Streaming}
	\caption{Diagramma delle classi relative alla cattura video}
	\label{fig:class_renderingSDLFull_Streaming}
\end{figure}

I listati precedenti sono stati estrapolati dal Lis. \ref{lst:draw13} disponibile in appendice \ref{chap:Listato}.



\subsection{Cattura audio} \label{subsec:cap3_Audio}
Come per la cattura video si può ricorrere all'hook di funzioni della libreria multimediale o alle API del sistema operativo per la gestione audio \parencite{GamingAnywhere}; ad esempio: "Windows Audio Session API" per Windows \parencite{WASAPI}, "Advanced Linux Sound Architecture" per Linux \parencite{ALSA} e "Core Audio" per macOS \parencite{Core_Audio_api}.

Come nel caso della cattura video ho scelto di modificare il sorgente del programma. Come mostrato in Fig. \ref{fig:class_mixingSDL_streaming} la classe \verb|sound_sdl| implementa la funzione di callback necessaria per utilizzare il missaggio audio di SDL, di cui abbiamo parlato nel paragrafo \ref{subsec:cap2_MissaggioAudio}. All'interno di questa funzione, tramite la funzione \verb|send_audio_interval|, è stato inviato alla classe che si occupa di gestire il server di streaming (\verb|streaming_server|) il buffer sonoro (\verb|stream|) e la sua dimensione (\verb|len|). La modifica apportata è mostrata nel Lis. \ref{lst:sdl_sound_callback}.

\begin{lstlisting}[caption=Codice aggiunto per la cattura audio, label={lst:sdl_sound_callback}]
	// 512 campioni audio (per canale)
	streaming_server::instance()
		.send_audio_interval(stream, len, 512);

	memset(stream, 0, len); // svuota buffer sonoro
\end{lstlisting}


\begin{figure}[H]
	\includegraphics[width=\linewidth]{immagini/class_mixingSDL_streaming}
	\caption{Diagramma delle classi relative alla cattura audio}
	\label{fig:class_mixingSDL_streaming}
\end{figure}

Il listato precedente è stato estrapolato dal Lis. \ref{lst:sdl_sound} disponibile in appendice \ref{chap:Listato}.




\section{Codifica} \label{sec:cap3_Codifica}
Nel paradigma dello streaming la codifica è un processo essenziale sia perché stabilisce il formato audiovisivo che verrà trasmesso e poi decodificato sia perché riduce l'elevata quantità di dati che compongono il flusso audiovisivo. La codifica avviene tramite un codificatore (hardware o software) che descrive e comprime i dati (solitamente con perdita dell'informazione). Di seguito parleremo dei codificatori video e audio.

\subsection{Codificatori video}
I codificatori video si divino in due gruppi:

\begin{itemize}
	\item intraframe: codificano ogni fotogramma indipendentemente dagli altri;
	\item interframe: descrivono i cambiamenti tra fotogrammi contigui; sono più efficienti su scene statiche poiché i cambiamenti da descrivere sono minori.
\end{itemize}

%YUV 4:2:0 planare da 12bpp (1 Cr & Cb campioni per 2x2 Y campioni)
Gli attuali monitor (CRT, LCD, PDP e OLED) utilizzano RGB come modello di colori (e di conseguenza gli attuali sistemi operativi), ma gli algoritmi di codifica video utilizzano il modello YCbCr\footnote{Il termine YUV è usato molto spesso come sinonimo di YCbCr. Si tratta tuttavia di formati differenti, uno analogico e l'altro digitale.}. Sfruttando l'innata sensibilità alla luminosità del sistema visivo umano questo modello suddivide i dati del colore in componenti separati di luminanza e crominanza, come mostrato in Fig. \ref{fig:RGB_YCbCr}, possiamo comprimere selettivamente solo i componenti di crominanza per ottenere risparmi di spazio con una perdita minima di qualità.

Utilizzando le costanti dello standard NTSC e PAL: $K_R$, $K_G$ e $K_B$ che valgono rispettivamente $0.299$, $0.587$ e $0.114$ \parencite{VideoAndMultimediaTransmissionsOverCellularNetworks}, la conversione da YCbCr a RGB avviene utilizzando l'Eq. \ref{eq:YUV_from_RGB}:

\begin{equation} \label{eq:YUV_from_RGB}
	\begin{aligned}
		& Y = K_R R + K_G G + K_B B, \\	
		& C_B = \frac{0.5}{1 - K_B} (B - Y), \\
		& C_R = \frac{0.5}{1 - K_R} (R - Y).	
	\end{aligned}
\end{equation}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{immagini/RGB_YCbCr}
	\caption{Componenti di RGB e YCbCr}
	\label{fig:RGB_YCbCr}
\end{figure}

Come mostrato in Fig. \ref{fig:RGB_YCbCr_matrix}, nel modello RGB i dati per un singolo componente di colore sono intercalati tra i pixel e ciascun pixel è archiviato in memoria in modo contiguo, mentre nel modello YCbCr le componenti Cb e Cr sono intercalate e memorizzate insieme, mentre la componente Y rimane nel proprio piano, per un totale di due piani.

\begin{figure}[H]
	\includegraphics[width=\linewidth]{immagini/RGB_YCbCr_matrix}
	\caption{Matrice di memorizzazione di RGB e di YCbCr}
	\label{fig:RGB_YCbCr_matrix}
\end{figure}

Come detto precedentemente la codifica può essere sia software che hardware, per quanto riguarda la codifica software nel paragrafo \ref{subsec:cap3_FFmpeg} parleremo della libreria FFmpeg che supporta la codifica e decodifica di tutti i codec presenti in Tab. \ref{table:CodecsVideo} e Tab. \ref{table:CodecsAudio}, invece per la codifica hardware è importante sottolineare quali soluzioni hardware hanno utilizzato le varie piattaforme di cloud gaming.

La prima società che modificò una scheda video adattandola per il cloud gaming fu Sony che incluse un chip di codifica sulle GPU Nvidia ed AMD (rispettivamente per il rendering PS3 e PS4) \parencite{PlayStation_Now_Chip}. Altre piattaforme di cloud gaming che usano schede video con funzionalità di encoding incluse sono "Nvidia GeForce Now" su schede Nvidia \parencite{GeForce_Now}, "Microsoft XBox Cloud Gaming" su schede AMD \parencite{xCloudBlade}, "Google Stadia" su AMD \parencite{Google_Stadia_Server} e "Amazon Luna" su Nvidia \parencite{Amazon_Luna_GPU}. La lista delle aziende che creano schede video con questa funzionalità è riportata in Tab. \ref{table:VideoCardEncodingFeature} \parencite{GCNArchitecture} \parencite{IntelQuickSyncVideo} \parencite{NVIDIAVideoCodecSDK} \parencite{Hexagon_DSP_SDK}.

\begin{table}[H]
	\centering
	\begin{tabular}{||c c c||} 
		\hline
		Produttore & Funzionalità & Codec supportati \\
		\hline\hline
		AMD & Video Core Next & AVC, HEVC \\
		\hline
		Intel & Quick Sync Video & AVC, HEVC, MPEG-2, VP8, VP9 \\
		\hline
		Nvidia & NVENC & AVC, HEVC \\
		\hline
		Qualcomm & Hexagon & AVC, H.263, HEVC, MPEG-4, VP8, VP9 \\
		\hline
	\end{tabular}

	\caption{Schede video con funzionalità di codifica video}
	\label{table:VideoCardEncodingFeature}
\end{table}

Una varietà di codec audio e video sono stati implementati fino ad oggi con licenza proprietaria o libera e caratteristiche diverse. Una lista dei codec video più utilizzati\footnote{I codec video più utilizzati sono di categoria interframe.} è mostrata in Tab. \ref{table:CodecsVideo} \parencite{WebVideoCodecGuide}.

\begin{table}[H]
	\centering
	\begin{tabular}{||c c c c c||} 
		\hline
		Codec & Contenitore & Perdita dei dati & Max bit-rate\tablefootnote{In Mbps.} & Licenza\tablefootnote{Alla scadenza dei brevetti il software può essere utilizzato liberamente.} \\
		\hline\hline
		AV1 & MP4, WebM & sì & 800 & libera \\
		\hline
		AVC & MP4 & sì/no & arbitrario & proprietaria \\
		\hline
		HEVC & MP4 & sì & 800 & proprietaria \\
		\hline
		MPEG-1 & MPEG & sì & 1,5 & scaduta \\
		\hline
		MPEG-2 & MP4, MPEG & sì & 100 & scaduta \\
		\hline
		Theora & Ogg & sì & 2000 & libera \\
		\hline
		VP8 & Ogg, WebM & sì & arbitrario & libera \\
		\hline
		VP9 & Ogg, WebM & sì & arbitrario & libera \\
		\hline
	\end{tabular}

	\caption{Lista codec video}
	\label{table:CodecsVideo}
\end{table}



\subsection{Codificatori audio}
Come per i codificatori video anche i codificatori audio sono di tipo hardware o software. I codificatori hardware sono integrati nelle schede audio e sono in grado di trasformare il segnale da analogico a digitale e viceversa, che solitamente è rappresentato digitalmente come modulazione a impulsi codificati (PCM) e non è compresso. La compressione audio avviene solo tramite codificatori software. Anche questi hanno tre formati di codifica: con perdita, senza perdita e nessuna compressione.

Il formato di codifica audio con perdita (l'unico usato per lo streaming) riduce la risoluzione in bit del suono e rimuove le frequenze che l'essere umano non può sentire o sente poco (secondo un modello psicoacustico). Altri fattori che influenzano la dimensione dell'audio codificato sono il numero di canali (che influisce solo sulla percezione della direzionalità e non sulla qualità) e il numero di campioni disponibili al secondo (che influiscono sulla fedeltà audio codificata) \parencite{AudioSignalProcessingAndCoding}.

%Ci sono vari algorimi base per la compressione di cui due sono maggiormente usati nei codificatori audio per lo streaming: \textit{Modified discrete cosine transform} (MDCT) e \textit{Sub-band coding} (SBC). La MDCT è una trasformata lineare ortogonale lappata, basata sull'idea della cancellazione dell'aliasing nel dominio del tempo. \parencite{AudioSignalProcessingAndCoding}.

Una lista dei codec audio più utilizzati è mostrata in Tab. \ref{table:CodecsAudio} \parencite{WebAudioCodecGuide}.

\begin{table}[H]
	\centering
	\begin{tabular}{||c c c c c||} 
		\hline
		Codec & Contenitore & Perdita dei dati & Max bit-rate\tablefootnote{In Kbps.} & Licenza\tablefootnote{Alla scadenza dei brevetti il software può essere utilizzato liberamente.} \\
		\hline\hline
		AAC & ADTS, MP4 & sì & 512 & proprietaria \\
		\hline
		ALAC & MP4 & no & variabile & proprietaria \\
		\hline
		FLAC & FLAC, MP4, Ogg & no & variabile & libera \\
		\hline
		MP2 & MPEG & sì & 320 & scaduta \\
		\hline
		MP3 & ADTS, MP4, MPEG & sì & 320 & scaduta \\
		\hline
		Opus & MP4, Ogg, WebM & sì & 510 & libera \\
		\hline
		PCM & WAV & no & 64 & scaduta \\
		\hline
		Vorbis & Ogg, WebM & sì & 500 & libera \\
		\hline
	\end{tabular}

	\caption{Lista codec audio}
	\label{table:CodecsAudio}
\end{table}


\subsection{La codifica tramite le librerie di FFmpeg} \label{subsec:cap3_FFmpeg}
Il progetto FFmpeg è un insieme di librerie e programmi open source per la gestione di video, audio, file e flussi. Include i codec per la codifica e la decodifica della maggior parte dei formati di file audio e video conosciuti \parencite{FFmpeg_Documentation}. FFmpeg include sette librerie per il C:

\begin{itemize}
	\item \verb|SW Resample| offre funzioni per il ricampionamento audio;
	\item \verb|SW Scale| offre funzioni per il ridimensionamento dell'immagine video e le routine di conversione dello spazio colore e del formato pixel;
	\item \verb|AV Codec| contiene tutti i codificatori e decodificatori audio/video FFmpeg nativi;
	\item \verb|AV Format| contiene demuxer e muxer per formati di contenitori audio e video;
	\item \verb|AV Util| è una libreria di supporto contenente routine comuni a diverse parti di FFmpeg, come funzioni hash, cifratura, decompressore LZO e codificatore/decodificatore Base64;
	\item \verb|AV Filter| permette di modificare o esaminare il video/audio tra il decoder e l'encoder;
	\item \verb|Post Proc| è una libreria contenente le vecchie routine di post-elaborazione video basate su h263.
\end{itemize}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{immagini/class_server_and_encoding}
	\caption{Diagramma delle classi relativo alla codifica}
	\label{fig:class_server_and_encoding}
\end{figure}

La classe che si occupa della codifica è \verb|encode_to_movie| che utilizza le seguenti librerie FFmpeg: \verb|SW Scale| per la conversione dal modello RGB a YCbCr, \verb|SW Resample| per il ricampionamento audio, \verb|AV Codec| per la codifica audio in MP2 e video in MPEG-1 e \verb|AV Format| per l'incapsulamento in MPEG-TS.

Come mostrato in Fig. \ref{fig:class_server_and_encoding} il costruttore di \verb|encode_to_movie| richiede il parametro \verb|socket| su cui verrà inviato il filmato al client. Tramite le funzioni \verb|add_frame| e \verb|add_instant| vengono aggiunti il frame video e quello audio per essere codificati.

Al termine di ogni fase di incapsulamento la libreria \verb|AV Format| invia alla funzione di callback \verb|encode_to_movie::write_packet| il pacchetto (\verb|buf| di lunghezza \verb|buf_size|) appena codificato.



\subsection{MPEG}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\subsubsection{Compression}
Il codec è un modo per comprimere il video non elaborato prima dello streaming; la maggior parte dei servizi di streaming non si occupa della compressione a livello di codec e funziona solo con protocolli e trasporti. Alcuni esempi di codec sono h264, aac, mp3.

Generalmente, il processo di conversione di segnali video compressi in altri segnali video è chiamato transcodifica video. Di solito, l'obiettivo principale di questa procedura è adattare le funzionalità video come bit rate, risoluzione o codec, per soddisfare i requisiti dei canali di comunicazione e dei dispositivi endpoint. È composto principalmente da compressione e decompressione di segnali video con tecniche di codifica video. La compressione è spesso chiamata codifica e la decompressione è indicata come decodifica \parencite{CombinedICTTechnologies}.

\subsubsection{Video}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\subsubsection{Audio}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\subsubsection{Trasporto}
Il trasporto o il contenitore definisce il modo in cui il video compresso viene compresso in byte per la trasmissione da una parte all'altra (utilizzando un protocollo). Ad esempio, MPEG-TS e RTMP sono contenitori \parencite{CombinedICTTechnologies}.




\section{Trasmissione}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\subsection{Web APIs}
Le API Web sono un insieme di API e interfacce che comprendono la potente capacità di creazione di script del Web. A seguire quelli utilizzati in questo progetto \parencite{Web_APIs}.

\subsubsection{WebSocket}
WebSocket è un protocollo di comunicazione del computer che fornisce canali di comunicazione full-duplex su una singola connessione TCP. È compatibile con HTTP perché l'handshake WebSocket utilizza l'intestazione di aggiornamento HTTP per passare dal protocollo HTTP al protocollo WebSocket. È supportato nativamente da tutti i browser e il suo utilizzo è simile ai normali socket sia sul lato client che su quello server. Per questi motivi è il protocollo di comunicazione generico più utilizzato sul web \parencite{WebSocket_Web_APIs}.




\section{Decodifica}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\subsection{Web APIs}
Le API Web sono un insieme di API e interfacce che comprendono la potente capacità di creazione di script del Web. A seguire quelli utilizzati in questo progetto \parencite{Web_APIs}.

\subsubsection{Canvas API}
L'API Canvas fornisce un mezzo per disegnare grafica tramite JavaScript, si concentra principalmente sulla grafica 2D ma quando viene utilizzata dall'API WebGL può disegnare grafica 2D e 3D con accelerazione hardware. È completamente supportato da tutti i browser \parencite{Canvas_API}.

\subsubsection{WebGL API}
WebGL è un'API JavaScript, progettata e gestita dal gruppo no-profit Khronos, per il rendering di grafica 2D e 3D che consente l'utilizzo accelerato dalla GPU della fisica e dell'elaborazione e degli effetti delle immagini. WebGL 1.0 è supportato su tutti i browser, mentre WebGL 2.0 viene testato su Safari \parencite{WebGL}.



\subsection{Librerie JavaScript}
Per il front-end, sono state utilizzate una libreria JavaScript open source per la decodifica del filmato.

\subsubsection{JSMpeg}
JSMpeg è una libreria composta da un demuxer MPEG-TS, un decoder video MPEG1 e audio MP2, con un sistema di rendering basato sia su WebGL che su Canvas2D, ed un sistema di output audio basato su WebAudio. Consente lo streaming a bassa latenza ($\sim$50ms) tramite WebSocket, ed è rilasciata con licenza MIT \parencite{JSMpeg}.




\section{Gestione input}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\subsection{Librerie JavaScript}
Per il front-end, sono state utilizzate due librerie JavaScript open source per la gestione degli input.

\subsubsection{Keypress}
Keypress è una libreria per la cattura dell'input da tastiera specializzata per l'uso in contesti videoludici, rilasciata con licenza Apache 2.0. Viene utilizzata per gestire l'input da tastiera nel front-end \parencite{Keypress}.

\subsubsection{GameController.js}
GameController.js è una libreria che estende le Web API per il gamepad, è rilasciata con licenza MIT. Nel front-end viene utilizzata per gestire i gamepads, per consentire il multiplayer sullo stesso dispositivo \parencite{gameController_js}.


\subsection{SDL input}
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

%\parencite{CPP_Primer}
%\parencite{Computer_Networking_and_the_Internet}
%\parencite{Ingegneria_del_software}
%\parencite{Understanding_the_Linux_Kernel}
%\parencite{Windows_Server_2012}